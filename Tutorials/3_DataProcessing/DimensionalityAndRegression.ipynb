{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace4257-ffc8-480f-bfe2-3ae08a8f8f58",
   "metadata": {},
   "source": [
    "# Simple Model Management; Dimensionality Reduction and Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a4527-2571-43c9-a33a-9a335b3b0247",
   "metadata": {},
   "source": [
    "## Preparing this Workspace\n",
    "\n",
    "For this tutorial, please set up a directory on TALC JupyterHub and upload the following files (all available on D2L) into it:\n",
    "\n",
    "* `DimensionalityAndRegression.ipynb`\n",
    "* `diabetes.csv`\n",
    "* `iris.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63128b65-c929-4767-8fe2-bd444618589b",
   "metadata": {},
   "source": [
    "## SciKit-Learn API Style\n",
    "\n",
    "For this tutorial, we will use the utilities provided by SciKit-learn. For the sake of time, we will only be touching on the simpler, common utilities provided by this package; links to more in-depth discussion of the full utilities available will be linked where applicable.\n",
    "\n",
    "For nearly all utilities in SciKit-Learn, we must first declare an *instance* of the utility. This instance is what we will actually use to analyze and/or transform the data, and how we declare it will determine how it behaves. This declaration is a function, taking a number of parameters which are stored within the instance and used when it processes the data its given. For example, the `SimpleImputer` utility (which replicates the imputation functionality we discussed last week) can take a `strategy` argument, which specifies how it should fill in non-existant values. We will discuss these arguments as they arise.\n",
    "\n",
    "Once declared, almost all SciKit-Learn utilities follow a common pattern in how they are used. This is accomplished via the use of shared functions which all utilities share, allowing for most forms of analysis post-utility initialization to be very straightforward:\n",
    "\n",
    "* `fit`: Requests that the utility \"analyzes\" the provided dataset, in preparation for its application. This can be as simple as checking against a condition, or as advanced as learning complex relations within a set of features in labels. This returns a (now updated) version of the utility that called it.\n",
    "* `transform`: Requests that the utility now applies what it has learned to the provided dataset. This dataset can be the one it originally analyzed, but does not have to be! It can instead be another dataset that shares the same features as the one it originally learned from. This returns the results of this application, as a modified copy of the original dataset; *the original dataset is not modified in this process*\n",
    "* `fit_transform`: Performs the `fit` and `transform` functions above, in that order, on the same dataset. Returns the resulting (modified) dataset. If the utility lacks a `transform` function, it will also lack this function.\n",
    "* `predict`: Like `transform` before it, this requests that the utility take the provided data and predict the correct target labels for them. This dataset does not need to be the one it originally learned from; it can be a different dataset that shares the same features instead. The predicted target labels are returned to you, for further analysis. All utilities will have `transform` or `predict`, but not both.\n",
    "* `score`: Evaluates how effectively the utility performs, usually with a higher value being better, when given a set of input features and their \"true\" label values. The metric for this evaluation differs depending on the utility, though this is usually either a form of accuracy or measure of error rate. Note that most utilities that do not \"learn\" parameters will not have this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740120d-b71e-4623-b530-0a3606597a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is Data Dimensionality?\n",
    "\n",
    "Values within a dataset, much like co-ordinates on a piece of paper, are often used to represent a \"position\" of entries within the full range of available data. As such, features within a dataset are often reffered to interchangeably as a \"dimension\" within a dataset, resulting in the number of features a dataset has being used to describe how many dimensions it has. Approaching data this way allows us to better grasp how data can be worked with, and how we can transform it to better suit our needs.\n",
    "\n",
    "However, just like a 20 dimensional plot is near impossible to understand and interpret, data with too many dimensions can be come unweildy to work with and lead to confusion. Due to how machine learning systems are designed, they are prone to a number of errors when fed with high dimensional data:\n",
    "\n",
    "1. Assuming all dimensions are equally worth using, even if some are completely irrelevant to the task at hand (similar to a \"red herring\" in a mystery novel; this information was given to me for a reason, right?)\n",
    "2. Treating all dimensions are effectively unique, even though multiple dimensions in the dataset may functionally represent the same thing (i.e. hours of daylight and solar power production likely both represent the amount of light in an environment)\n",
    "3. Accidentally implying relations where there are none (ice cream sales and murder rate are correlated to one another; in reality, they just both happen to increase in the summer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75e248-ed60-4321-95bc-b0804657426c",
   "metadata": {},
   "source": [
    "To combat these errors, a number of \"dimensionality reduction\" techniques have been devised. Thankfully, these problems are common enough that utilities to automate these processes have been coded for us.\n",
    "\n",
    "For this tutorial, we will be using modified version of two datasets; the [iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) dataset (which provides us with a species classification task), and the [diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes) dataset (which provides us with a disease progression task instead). Note that the latter has had its data normalized, resulting the seemingly nonsensical values present within some of its columns. Both have also been scrambled, and had a new \"id\" metadata column added to them for identification purpouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322b1ea5-ab9f-4bb6-9582-29e14e19383d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load our iris data\n",
    "iris_df = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "# Load our diabetes data\n",
    "diab_df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57448eef-284a-4746-9632-f07d0190da2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>843</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>851</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>857</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>865</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>867</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "0      3                6.7               3.1                4.7   \n",
       "1     13                5.7               2.8                4.1   \n",
       "2     20                6.1               3.0                4.6   \n",
       "3     24                6.7               3.3                5.7   \n",
       "4     29                5.4               3.0                4.5   \n",
       "..   ...                ...               ...                ...   \n",
       "145  843                4.6               3.4                1.4   \n",
       "146  851                6.4               2.7                5.3   \n",
       "147  857                5.1               3.8                1.9   \n",
       "148  865                5.7               3.8                1.7   \n",
       "149  867                5.9               3.0                4.2   \n",
       "\n",
       "     petal width (cm)  species  \n",
       "0                 1.5        1  \n",
       "1                 1.3        1  \n",
       "2                 1.4        1  \n",
       "3                 2.1        2  \n",
       "4                 1.5        1  \n",
       "..                ...      ...  \n",
       "145               0.3        0  \n",
       "146               1.9        2  \n",
       "147               0.4        0  \n",
       "148               0.3        0  \n",
       "149               1.5        1  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fd3825-1131-4c3e-8d55-031766bb4247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>response of interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2432</td>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2433</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2437</td>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2440</td>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2446</td>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       age       sex       bmi        bp        s1        s2  \\\n",
       "0       8  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821   \n",
       "1      12 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163   \n",
       "2      21  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194   \n",
       "3      29 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991   \n",
       "4      35  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "437  2432  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566   \n",
       "438  2433 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165   \n",
       "439  2437  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840   \n",
       "440  2440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283   \n",
       "441  2446 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809   \n",
       "\n",
       "           s3        s4        s5        s6  response of interest  \n",
       "0   -0.043401 -0.002592  0.019908 -0.017646                 151.0  \n",
       "1    0.074412 -0.039493 -0.068330 -0.092204                  75.0  \n",
       "2   -0.032356 -0.002592  0.002864 -0.025930                 141.0  \n",
       "3   -0.036038  0.034309  0.022692 -0.009362                 206.0  \n",
       "4    0.008142 -0.002592 -0.031991 -0.046641                 135.0  \n",
       "..        ...       ...       ...       ...                   ...  \n",
       "437 -0.028674 -0.002592  0.031193  0.007207                 178.0  \n",
       "438 -0.028674  0.034309 -0.018118  0.044485                 104.0  \n",
       "439 -0.024993 -0.011080 -0.046879  0.015491                 132.0  \n",
       "440 -0.028674  0.026560  0.044528 -0.025930                 220.0  \n",
       "441  0.173816 -0.039493 -0.004220  0.003064                  57.0  \n",
       "\n",
       "[442 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cefebf2-e8fc-4460-a303-17e41836b18b",
   "metadata": {},
   "source": [
    "Before doing any analysis which involves feature modification, we generally want to split the dataset's columns three ways:\n",
    "\n",
    "* The metadata columns (which label the information for the purpouses of its management, but have little to no signficance in how we actually interpret the data)\n",
    "* The feature columns (which do hold relevant information on how we interpret the dataset, informing any analyses)\n",
    "* The label/target columns (which are the column(s) we intend to design our analyses around interpretting)\n",
    "\n",
    "Examples of how you might want to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22968dd2-ab40-4667-a369-caffa188c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris data preparation w/ labels (loc)\n",
    "iris_meta = iris_df.loc[:, 'id']\n",
    "iris_features = iris_df.loc[:, 'sepal length (cm)':'petal width (cm)']\n",
    "iris_target = iris_df.loc[:, 'species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4a59b4-73f3-47b5-9e2b-0d112bf62678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes data preparation w/ positions (iloc)\n",
    "diab_meta = diab_df.iloc[:, 0]\n",
    "diab_features = diab_df.iloc[:, 1:11]\n",
    "diab_target = diab_df.iloc[:, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6b637-9f75-47f7-a098-8a7eb960cd44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5aa936-81a9-4b84-8d6d-5b86b2de41c4",
   "metadata": {},
   "source": [
    "### Option 1; Feature Selection\n",
    "\n",
    "This simplest way to reduce the number of dimensions in a dataset is as straightforward as can be; just start throwing away stuff we don't want! Depending on your circumstances, we can accomplish this one of two ways (though these can also be combined if needed):\n",
    "\n",
    "1. Unsupervised feature selection (select features based on metrics implicit to the features themselves)\n",
    "2. Supervised feature selection (select features based on a metric that represents how we would expect them to impact the final target model)\n",
    "\n",
    "SciKit-Learn has a well developed suite of utilities to handle both of these circumstances; for this tutorial, we will only touch on the simple methods they provide. An in-depth discussion of the full utilities available can be accessed [here](https://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69899e8b-7dae-493b-a2d1-50262b8e9fa1",
   "metadata": {},
   "source": [
    "#### 1. Unsupervised Example; Selection by Variance\n",
    "\n",
    "A simple unsupervised option is to remove features with low *variance*. This is based on the idea that features that have low variation are likely only represent error and/or uninformative noise. For this, we can use the `VarianceThreshold` utility to automate the process. To instantiate it, one parameter is needed:\n",
    "\n",
    "* `threshold`: The minimum variance that is required for a feature to be conserved. Remember variance is the square of standard deviation (which measures the average distance from the mean), so this value may need to be square-rooted to acomplish your intended goal. This value will also scale depending on the average scale of the features being used, with ).\n",
    "\n",
    "We will apply this to the iris dataset, using `0.25` as our variance (chosen as the data is measured in centimeters, and samples differing by less than 0.5cm on average are likely the result of random genetic differences rather than species specific ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8644112f-9064-4ae5-929b-fe4abdbb619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "feature_selector = VarianceThreshold(threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96a745-a1ae-48ec-95b6-e4ea8f5d5503",
   "metadata": {},
   "source": [
    "Once this is completed, we can apply it to our datasets using the shared functions mentioned prior. Note that this returns a multi-dimensional array, *not* a DataFrame, so we need to convert it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762e1fbb-18a8-40e2-a22f-e55ce3f8e27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  petal length (cm)  petal width (cm)\n",
       "0                  6.7                4.7               1.5\n",
       "1                  5.7                4.1               1.3\n",
       "2                  6.1                4.6               1.4\n",
       "3                  6.7                5.7               2.1\n",
       "4                  5.4                4.5               1.5\n",
       "..                 ...                ...               ...\n",
       "145                4.6                1.4               0.3\n",
       "146                6.4                5.3               1.9\n",
       "147                5.1                1.9               0.4\n",
       "148                5.7                1.7               0.3\n",
       "149                5.9                4.2               1.5\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "feature_selector.fit(iris_features)\n",
    "trimmed_iris_features = feature_selector.transform(iris_features)\n",
    "\n",
    "# Rebuild a DataFrame from the results\n",
    "feature_names = feature_selector.get_feature_names_out()\n",
    "trimmed_iris_features = pd.DataFrame(data=trimmed_iris_features, columns=feature_names)\n",
    "\n",
    "trimmed_iris_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27ca2d-0e78-4ec6-9d38-fc6928429e45",
   "metadata": {},
   "source": [
    "Nice; only one feature of the iris dataset was had too little variation, resulting in 3 of our 4 features being maintaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f06a0-7ff8-4804-b175-3c1a80773e5d",
   "metadata": {},
   "source": [
    "#### 2. Supervised Example; `f_regression` guided K-Best\n",
    "\n",
    "Alternatively, we can select a feature based on how \"good\" it at aiding our prediction task. Most methods in SciKit-Learn allow us to define how this is measured, as needed; depending on whether our target is categorical or continuous, a number of functions exist to do so:\n",
    "\n",
    "* For continuous: `f_regression` (F-Test on continous targets) and `mutual_info_regression` (measure of how dependent the target is on input feature values)\n",
    "* For categorical: `chi2` (chi-squared test), `f_classif` (F-Test on categorical targets), and `mutual_info_classif` (measure of how dependent the target is on input feature class)\n",
    "\n",
    "Similarly, we have options for how we want to determine the cutoff for \"goodness\" of our features:\n",
    "\n",
    "* `SelectKBest`: Select the an explicit number (`k`) of the best features in the dataset\n",
    "* `SelectPercentile`: Select best percentage of features in the dataset, rounded to the nearest whole number.\n",
    "\n",
    "Both utilities requires two arguments when declared\n",
    "\n",
    "* `score_func`: The scoring function to apply to the method; these methods assume higher values are preferred to lower ones!\n",
    "* `k`/`percentile`: The amount of features to preserve (respectively as a whole number or decimal percentile for `KBest` and `Percentile` respectively)\n",
    "\n",
    "These features, being supervised, also need our target metric to be provided to them as a second parameter when the `fit` command is called. Below is an example of this using our diabetes dataset, with `SelectKBest` with an `f_regression` scoring system and preserving the top 5 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b630b61f-fc9b-469e-bed5-43ea595be6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          bmi        bp        s3        s4        s5\n",
       "0    0.061696  0.021872 -0.043401 -0.002592  0.019908\n",
       "1   -0.051474 -0.026328  0.074412 -0.039493 -0.068330\n",
       "2    0.044451 -0.005671 -0.032356 -0.002592  0.002864\n",
       "3   -0.011595 -0.036656 -0.036038  0.034309  0.022692\n",
       "4   -0.036385  0.021872  0.008142 -0.002592 -0.031991\n",
       "..        ...       ...       ...       ...       ...\n",
       "437  0.019662  0.059744 -0.028674 -0.002592  0.031193\n",
       "438 -0.015906 -0.067642 -0.028674  0.034309 -0.018118\n",
       "439 -0.015906  0.017282 -0.024993 -0.011080 -0.046879\n",
       "440  0.039062  0.001215 -0.028674  0.026560  0.044528\n",
       "441 -0.073030 -0.081414  0.173816 -0.039493 -0.004220\n",
       "\n",
       "[442 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the feature selection utility\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# Import the scoring function for it to use\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Initialize the utility\n",
    "feature_selector = SelectKBest(score_func=f_regression, k=5)\n",
    "\n",
    "# Fit and transform our diabetes dataset\n",
    "feature_selector.fit(diab_features, diab_target)\n",
    "trimmed_diab_features = feature_selector.transform(diab_features)\n",
    "\n",
    "# Rebuild a DataFrame from the results\n",
    "feature_names = feature_selector.get_feature_names_out()\n",
    "trimmed_diab_features = pd.DataFrame(data=trimmed_diab_features, columns=feature_names)\n",
    "\n",
    "trimmed_diab_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2557e8f-eef6-464d-abbd-4d785bedb496",
   "metadata": {},
   "source": [
    "As requested, only 5 features remain, selected to be the best performing based on a continuous F-test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df8977-1218-48f0-a9a8-e0e07a4af7ba",
   "metadata": {},
   "source": [
    "<img style=\"float: left; padding:10px;\" src=\"https://img.icons8.com/office/344/faq.png\" width=80 height=80 />\n",
    "\n",
    "**Consider the Following**\n",
    "\n",
    "Which of the original three issues mentioned prior does this method solve? \n",
    "\n",
    "Are their potential problems that doing dimensionality reduction this way may introduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e952a-3369-46ea-8341-a9c9f78459c7",
   "metadata": {},
   "source": [
    "### Option 2; Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2eaf3-5215-4fa8-805d-e10c1c6f8d12",
   "metadata": {},
   "source": [
    "Instead of deleting features entirely, we can transform them into new, more condensed features instead. Like before, we can do this two ways:\n",
    "\n",
    "* Unsupervised; try to preserve as much variation as possible.\n",
    "* Supervised; try to preserve as much *relevant* variation as possible, based on how they appear to relate to the target feature(s).\n",
    "\n",
    "While alternatives exist, by far and away the most common approaches for each of these is **Principal Component Analysis (PCA)** and **Linear Discriminant Analysis (LDA)**, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291aa46-6719-4bd6-9ca2-1b47f7175772",
   "metadata": {},
   "source": [
    "#### 1. Unsupervised Example; PCA\n",
    "\n",
    "To quickly review, PCA attempts to transform the provided features into *components*, each representing as much variation as possible within the full dataset. This means the first component is produces represents the most variation, followed by the second, and so on. As it measures variation, you should aim to have each feature share the same approximate scale; otherwise features which happen to be scaled lower than others (by centimeters rather than meters, for example) may be ignored by PCA simply because their variance is numerically lower. In SciKit-Learn, we can run this transformation using the `PCA` utility. When we initialize this utility, you may specify the following argument:\n",
    "\n",
    "* `n_components`: The number of Principal Components that should be kept. If not specified, will default to either the number of features you provided minus 1, or the number of samples you provided, whichever is smaller.\n",
    "\n",
    "An example of this process is shown below, reducing the diabetes dataset down to 5 features again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66facdba-2b2a-41b4-a0fc-c77640c17b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc0</th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027931</td>\n",
       "      <td>-0.092601</td>\n",
       "      <td>0.028027</td>\n",
       "      <td>-0.003939</td>\n",
       "      <td>-0.012207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.134686</td>\n",
       "      <td>0.065263</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.022356</td>\n",
       "      <td>-0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012945</td>\n",
       "      <td>-0.077764</td>\n",
       "      <td>0.035164</td>\n",
       "      <td>-0.037647</td>\n",
       "      <td>-0.055357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>-0.095750</td>\n",
       "      <td>0.065318</td>\n",
       "      <td>0.012154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.035981</td>\n",
       "      <td>0.038621</td>\n",
       "      <td>-0.002724</td>\n",
       "      <td>-0.006541</td>\n",
       "      <td>-0.006343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.058958</td>\n",
       "      <td>-0.049275</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>-0.031215</td>\n",
       "      <td>0.009718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.036211</td>\n",
       "      <td>-0.083249</td>\n",
       "      <td>-0.053914</td>\n",
       "      <td>-0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.009763</td>\n",
       "      <td>-0.057337</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>-0.064372</td>\n",
       "      <td>-0.006739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>0.076903</td>\n",
       "      <td>0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.090561</td>\n",
       "      <td>0.189108</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>-0.010493</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pc0       pc1       pc2       pc3       pc4\n",
       "0    0.027931 -0.092601  0.028027 -0.003939 -0.012207\n",
       "1   -0.134686  0.065263  0.001328 -0.022356 -0.006813\n",
       "2    0.012945 -0.077764  0.035164 -0.037647 -0.055357\n",
       "3    0.002345  0.018182 -0.095750  0.065318  0.012154\n",
       "4   -0.035981  0.038621 -0.002724 -0.006541 -0.006343\n",
       "..        ...       ...       ...       ...       ...\n",
       "437  0.058958 -0.049275  0.044173 -0.031215  0.009718\n",
       "438  0.060155  0.036211 -0.083249 -0.053914 -0.004472\n",
       "439 -0.009763 -0.057337  0.023596 -0.064372 -0.006739\n",
       "440  0.032956  0.009994 -0.041321  0.076903  0.005691\n",
       "441 -0.090561  0.189108 -0.002301 -0.010493  0.028531\n",
       "\n",
       "[442 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Transforming our data\n",
    "pca = PCA(n_components=5)\n",
    "pca_diab_features = pca.fit_transform(diab_features)\n",
    "\n",
    "# Restoring it to dataframe format\n",
    "col_names = []\n",
    "for i in range(5):\n",
    "    col_names.append(f\"pc{i}\")\n",
    "\n",
    "pca_diab_features = pd.DataFrame(data=pca_diab_features, columns=col_names)\n",
    "\n",
    "pca_diab_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e09f9-9cb2-4d99-b13e-69a80a5624c1",
   "metadata": {},
   "source": [
    "Note that we can also inspect the PCA utility itself to see how much variation each component \"explains\" from the original feature set by querying it's `explained_variance_ratio_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d25b41-3703-4286-99d4-013f0ee4043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40242142, 0.14923182, 0.12059623, 0.09554764, 0.06621856])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd41ae-67fe-4f59-8a55-4c2c483e4881",
   "metadata": {},
   "source": [
    "The above tells us the first principal component represents ~40.24% of the original datasets variation, the second ~14.92%, and so on. The total variance represented can be calculated simply by taking the sum of this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075456d2-28de-4e9b-b5fd-64f55cc96112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8340156689459763"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1b1fd-64b5-4c6c-98c1-d533648c48f2",
   "metadata": {},
   "source": [
    "83.4% of the dataset's variation in 50% of the features; not to shabby!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d07e7-8fc7-4f98-8705-69fb552b8786",
   "metadata": {},
   "source": [
    "#### 2: Supervised Example; LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a706e-5d90-4be5-b791-f20fba8af8d0",
   "metadata": {},
   "source": [
    "Similar to PCA prior, LDA aims to reduce the number of features by transforming them. However, unlike PCA, the method of doing so aims to preserve only informative variation, based on a set of provided target labels. Despite this change, LDA requires similar considerations for the data; namely, since it is still aiming to preserve variation, standardizing the scale of each feature is important to prevent issues from cropping up. In SciKit-Learn, this transformation is handled by the `LinearDiscriminantAnalysis` utility (`LDA` was already taken up by another utility, hence no acronym). We can get around this using the same technique of renaming `pandas` to `pd`, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741db326-7239-4fe6-9e1f-e4333ca4b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba339f-53b1-492f-9123-816f0141cdc1",
   "metadata": {},
   "source": [
    "Like PCA, you can specify the following argument to tailor its processing to your needs:\n",
    "\n",
    "* `n_components`: The number of Linear Discriminants that should be kept. If not specified, will default to either the number of features you provided minus 1, or the number of samples you provided, whichever is smaller.\n",
    "\n",
    "Below is the PCA analysis done prior, replicated with LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbabd7fe-c1dd-4f6a-8976-fac3a3b52cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ld0</th>\n",
       "      <th>ld1</th>\n",
       "      <th>ld2</th>\n",
       "      <th>ld3</th>\n",
       "      <th>ld4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.663158</td>\n",
       "      <td>-1.367074</td>\n",
       "      <td>-0.715150</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>-0.300979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.312019</td>\n",
       "      <td>0.724032</td>\n",
       "      <td>-1.024317</td>\n",
       "      <td>-1.088293</td>\n",
       "      <td>-1.718292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750935</td>\n",
       "      <td>-1.952337</td>\n",
       "      <td>-0.414892</td>\n",
       "      <td>-0.697182</td>\n",
       "      <td>-0.940487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.425156</td>\n",
       "      <td>0.924826</td>\n",
       "      <td>1.281073</td>\n",
       "      <td>0.399299</td>\n",
       "      <td>1.288591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.760852</td>\n",
       "      <td>0.154349</td>\n",
       "      <td>-1.057222</td>\n",
       "      <td>-1.435059</td>\n",
       "      <td>0.379962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1.253643</td>\n",
       "      <td>-1.214335</td>\n",
       "      <td>-1.384590</td>\n",
       "      <td>0.339403</td>\n",
       "      <td>0.592753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-1.514726</td>\n",
       "      <td>-1.279373</td>\n",
       "      <td>2.298311</td>\n",
       "      <td>0.521716</td>\n",
       "      <td>0.622083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.810260</td>\n",
       "      <td>-1.568301</td>\n",
       "      <td>-0.436171</td>\n",
       "      <td>-0.532425</td>\n",
       "      <td>0.469731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>1.684606</td>\n",
       "      <td>0.659046</td>\n",
       "      <td>0.177554</td>\n",
       "      <td>0.065803</td>\n",
       "      <td>0.280087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-3.092283</td>\n",
       "      <td>3.940658</td>\n",
       "      <td>1.356366</td>\n",
       "      <td>1.153056</td>\n",
       "      <td>-1.897223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ld0       ld1       ld2       ld3       ld4\n",
       "0    1.663158 -1.367074 -0.715150  0.001265 -0.300979\n",
       "1   -2.312019  0.724032 -1.024317 -1.088293 -1.718292\n",
       "2    0.750935 -1.952337 -0.414892 -0.697182 -0.940487\n",
       "3    0.425156  0.924826  1.281073  0.399299  1.288591\n",
       "4   -0.760852  0.154349 -1.057222 -1.435059  0.379962\n",
       "..        ...       ...       ...       ...       ...\n",
       "437  1.253643 -1.214335 -1.384590  0.339403  0.592753\n",
       "438 -1.514726 -1.279373  2.298311  0.521716  0.622083\n",
       "439 -0.810260 -1.568301 -0.436171 -0.532425  0.469731\n",
       "440  1.684606  0.659046  0.177554  0.065803  0.280087\n",
       "441 -3.092283  3.940658  1.356366  1.153056 -1.897223\n",
       "\n",
       "[442 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming our data\n",
    "lda = LDA(n_components=5)\n",
    "lda_diab_features = lda.fit_transform(diab_features, diab_target)\n",
    "\n",
    "# Restoring it to dataframe format\n",
    "col_names = []\n",
    "for i in range(5):\n",
    "    col_names.append(f\"ld{i}\")\n",
    "\n",
    "lda_diab_features = pd.DataFrame(data=lda_diab_features, columns=col_names)\n",
    "\n",
    "lda_diab_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80209b00-9353-4333-ba72-8cf2af08dd0b",
   "metadata": {},
   "source": [
    "Identically to PCA, the explained variance of the Linear Discriminants can also be analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82cb1554-6a63-4722-9c55-59f018de5681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2520478 , 0.11323441, 0.11121644, 0.09763492, 0.09338717])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50884fe6-9926-42f1-970e-42d74d84d535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675207483488869"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lda.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18015d19-1cc7-4306-a065-a9e5d89db136",
   "metadata": {},
   "source": [
    "Notice that this has dropped compared to PCA; while we are still improving on the variance-to-features ratio (66.8% for 50% features of not bad at all), the restriction that only *relevant* variation is preserved reduces the amount of overall variation conserved in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f3f4f-dce6-451a-a38c-038a8f5fa86a",
   "metadata": {},
   "source": [
    "<img style=\"float: left; padding:10px;\" src=\"https://img.icons8.com/office/344/faq.png\" width=80 height=80 />\n",
    "\n",
    "**Consider the Following**\n",
    "\n",
    "Which of the original three issues mentioned prior do these methods solve? \n",
    "\n",
    "Are their potential problems that doing dimensionality reduction using these methods may introduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c29bed-2224-43fa-abb4-1c15c3d3e20e",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa50338-a66d-4c57-8dfe-5261fe697111",
   "metadata": {},
   "source": [
    "Now that our data is finally prepared, we can move to actually interpret it. Regression is one of the simplest methods for doing so, as it only attempts to identify \"relations\" btween features and the target, without considering any other factors. Depending on the type of data we are working on, one of two common forms of regression exist:\n",
    "\n",
    "* `LinearRegression`; for target metrics which are continuous\n",
    "* `LogisticRegression`; for target metrics which are categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962161f-1137-4d37-b324-6808f6f68d23",
   "metadata": {},
   "source": [
    "Each of these do not need any arguments to initialize, but the following can be specified if you would like to tune them.\n",
    "\n",
    "* `n_jobs`: Defaults to `1`, but can be set higher if you are running on a machine that has mutliple available CPU cores that can be run in parralel. Can be set to `-1` to use all available cores if you are not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417acd25-8f0a-4058-8a7b-2f6ead8b6682",
   "metadata": {},
   "source": [
    "### Logistic Regression Example\n",
    "\n",
    "Lets first run our regression on the trimmed Iris data to see how it works. Our target is the `species` of the Iris, which is categorical, so we will use `LogisticRegression` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca194c79-58bb-43a8-9311-320c9a3b8c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# Fit it to our data\n",
    "logreg.fit(trimmed_iris_features, iris_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33adcbf6-41af-4bdb-b7ed-984e9f4445ba",
   "metadata": {},
   "source": [
    "Once a model is \"trained\" like this, it can then be used to predict the target class for data containing the same features (for the sake of simplicity, this is just a subset of the full iris dataset, but any data works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f994bf-fa1b-42a6-8dcc-0d88ae2eda1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  True\n",
       "0           1     1\n",
       "1           1     1\n",
       "2           1     1\n",
       "3           2     2\n",
       "4           1     1\n",
       "5           1     1\n",
       "6           1     2\n",
       "7           0     0\n",
       "8           0     0\n",
       "9           0     0\n",
       "10          0     0\n",
       "11          1     1\n",
       "12          2     2\n",
       "13          1     1\n",
       "14          1     1\n",
       "15          2     2\n",
       "16          1     1\n",
       "17          2     2\n",
       "18          2     2\n",
       "19          1     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our subset\n",
    "iris_feature_subset = trimmed_iris_features.iloc[:20, :]\n",
    "iris_target_subset = iris_target.iloc[:20] # No column index because there is only one column\n",
    "\n",
    "# Predict the subset's label using the model we just trained\n",
    "predicted_labels = logreg.predict(iris_feature_subset)\n",
    "\n",
    "# Place these predictions in a dataframe alongside the \"true\" labels for comparison\n",
    "iris_comp_df = pd.DataFrame(data = [predicted_labels, iris_target_subset], index=['Predicted', 'True']).T # The T is just to make sure the data is orientated correctly\n",
    "\n",
    "iris_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37a71-920d-47d1-aefc-47e42f7dc4d3",
   "metadata": {},
   "source": [
    "Not bad! Looking over the data, only one mistake appears to have been made (row 6). We can use the `score` function instead (which just requires the features to build predictions off of, and the true labels to compare said predictions against) to get an numerical representation of this rate of error, should we not want to inspect the data manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53e6691d-90e1-4463-a411-f783a8d2c221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(iris_feature_subset, iris_target_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f8591-a30c-4605-9e64-a0719f7a410d",
   "metadata": {},
   "source": [
    "In the case of Logistic Regression, this score is reported as accuracy. Thus, our model had a 95% accuracy on our subset; not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1b539-b174-4504-a61b-c9a0396da3a6",
   "metadata": {},
   "source": [
    "### Linear Regression Example\n",
    "\n",
    "As our diabetes data is continuous, we instead need to use `LinearRegression` instead. The workflow is otherwise the same, though the final score is reported a little differently. This is demonstrated below with our LDA-transformed diabetes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af4cf07b-eac7-4569-a2cf-02e9cc5cc3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the model\n",
    "lda_linreg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Fit it to our data\n",
    "lda_linreg.fit(lda_diab_features, diab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b35d8d3-891c-4b2d-845d-20c41110e5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43223345760777465"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our evaluation subset\n",
    "diab_lda_feature_subset = lda_diab_features.iloc[:30, :]\n",
    "diab_target_subset = diab_target.iloc[:30] # No column index because there is only one column\n",
    "\n",
    "# Evaluate the model\n",
    "lda_linreg.score(diab_lda_feature_subset, diab_target_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433c549-5e49-4b44-afba-1bffdd5b523d",
   "metadata": {},
   "source": [
    "For linear regression, the score reported represents \"the coefficient of determination\" for the prediction. This is a measure of how much a predicted value deviates from the true target, with a value of `1.0` representing perfect prediction, and a score of `0.0` representing the accuracy of a completely neive system (one which just predicts the most common answer, without any further consideration). As such, our score of 0.43, while not ideal, is a good sign; the model is clearly making informed predictions and doing so with reasonable predictive accuracy. Let's check whether using our PCA data improves things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bef5d30-3f6b-4177-8a78-0b759e5c7b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-Initialize the model\n",
    "pca_linreg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Fit it to our data\n",
    "pca_linreg.fit(pca_diab_features, diab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9dd570-d0a8-452e-bb51-45e3a1730e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3398952724319929"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our evaluation subset\n",
    "diab_pca_feature_subset = pca_diab_features.iloc[:30, :]\n",
    "diab_target_subset = diab_target.iloc[:30] # No column index because there is only one column\n",
    "\n",
    "# Evaluate the model\n",
    "pca_linreg.score(diab_pca_feature_subset, diab_target_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3221e0-b8a7-444f-8304-1aac6fa1a718",
   "metadata": {},
   "source": [
    "Seems that, despite PCA conserving more variation, said variation actually hurts our results in this case, rather than improving them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc85c1-9b69-4402-a7e5-5e9bf888679f",
   "metadata": {},
   "source": [
    "<img style=\"float: left; padding:10px;\" src=\"https://img.icons8.com/office/344/faq.png\" width=80 height=80 />\n",
    "\n",
    "**Consider the Following**\n",
    "\n",
    "Do these relations represent causal interactions between the features and our target? If not, what do they represent instead?\n",
    "\n",
    "Are their types of relations which regression may fail to account for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9b311-24e4-4292-b45e-d3d143478ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDPR_Course",
   "language": "python",
   "name": "mdpr_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
