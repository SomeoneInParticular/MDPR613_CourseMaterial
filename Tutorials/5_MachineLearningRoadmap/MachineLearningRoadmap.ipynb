{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace4257-ffc8-480f-bfe2-3ae08a8f8f58",
   "metadata": {},
   "source": [
    "# Machine Learning Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a4527-2571-43c9-a33a-9a335b3b0247",
   "metadata": {},
   "source": [
    "## The Stages of Machine Learning Analysis\n",
    "\n",
    "For 99% of analyses which use machine learningm, the method of their use can be broken down into three stages\n",
    "\n",
    "1. **Data Preparation**: Importing the data, cleaning it, and identifying the features and target\n",
    "2. **Model Creation**: Selecting which type of model to use and training it with the data available\n",
    "3. **System Evaluation**: Testing the model to confirm that it is performing well in the ways we want it to\n",
    "\n",
    "Below is a template for each of these steps that you can fill in to help build up your machine learning analysis. Links to potential tools for each are also provided for your reference; most of these links have example code to work with, though you may need to scroll a bit to find it. A number of pre-built code snippets have also been created for you to copy-paste as a starting point as well, with all being available on D2L.\n",
    "\n",
    "Don't stress about the massive block of possible arguments that the documentation linked herein has. They are there to help programmers who want to integrate their code into a larger system, not the average user. For 99% of your use cases the usage provided by the example code (either by the documentation or ourselves) will be sufficient; we are simply providing it to provide context on these tools' full scope of applications, should you need to do more detail analyses in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e13e0-5b85-47ee-8af4-0540ac73115b",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "### Importing our Data\n",
    "\n",
    "The first step of preparing the data is to import it into our program. Pandas (`import pandas as pd`) is generally advised for this, and depending on the format of the data, different commands can be used:\n",
    "\n",
    "* `read_csv`: Imports comma-separated-value style data (usually with `.csv` or `.tsv` extensions). Often the default format for data, so if all else fails, try this. [Documentation Here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "* `read_excel`: Imports data created and saved using Microsoft Excel or similar. Has a large number of extensions depending on the Excel version used (`.XLS`, `.XLSX`, `.XLSM`, `.XLTX` and `.XLTM`), but pandas will usually figure out how to import it for you regardless. If your computer defaults to opening the data in excel without asking you anything, this is probably the command to use. [Documentation Here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "* `read_json`: Imports data stored in JavaScript-style format. These files are pretty rare outside of direct database or website queries, and are denoted with a `.json` extension. This is is most likely the format used if the prior two options fail. [Documentation Here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html)\n",
    "\n",
    "Other formats exist as well, though they are much more uncommon. If none of the above work for you, please contact us and we will figure out the formatting and convert the data into one of the above formats for your use instead.\n",
    "\n",
    "Place the code importing your data into the code block below. Place it within the `df` variable to keep consistent with subsequent blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ce756d-2418-453b-8bda-15a602f810d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ace707e-c567-4c70-a9b7-7c71362c2db9",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082ee1e-4716-47fb-a615-42fe009ba235",
   "metadata": {},
   "source": [
    "Before we know what needs to be changed about the data, we should inspect it first. First we should look into the distribution of the data; pandas DataFrames allow us to do this with `df.describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254b606f-b4af-4e5c-8ac8-b53f61bb84ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>424.173333</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>251.099642</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>217.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>420.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>624.500000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>867.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count  150.000000         150.000000        150.000000         150.000000   \n",
       "mean   424.173333           5.843333          3.057333           3.758000   \n",
       "std    251.099642           0.828066          0.435866           1.765298   \n",
       "min      3.000000           4.300000          2.000000           1.000000   \n",
       "25%    217.250000           5.100000          2.800000           1.600000   \n",
       "50%    420.500000           5.800000          3.000000           4.350000   \n",
       "75%    624.500000           6.400000          3.300000           5.100000   \n",
       "max    867.000000           7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)     species  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of our features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb78b3-f337-4987-affc-27918ed73a42",
   "metadata": {},
   "source": [
    "We should also check whether there are null (non-existant) values that we will need to deal with as well. This can be done with `df.isnull().sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf5e5b7-e58a-45b0-afc3-7a006f6ac6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "sepal length (cm)    0\n",
       "sepal width (cm)     0\n",
       "petal length (cm)    0\n",
       "petal width (cm)     0\n",
       "species              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether null values are present\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d15d76-2629-44ab-9e60-03afafe2a145",
   "metadata": {},
   "source": [
    "If there are any nulls values in the dataset, we should deal with them now before proceeding. We can do this one of two ways\n",
    "* Deleting entries will null values. This is good if null values are few and far between, resulting in data deletion having a negligable effect on the dataset as a whole. This can be accomplished with `df = df.dropna()`. The documentation for the method can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "* Imputing missing entries, \"filling them in\" with data generated from the rest of the dataset. For this `sklearn.impute.SimpleImputer` is generally sufficient, filling in missing values with average of the rest of the column by default. The documentation for this method can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "If you wish to apply either of these to your dataset, do so below. Make sure the result remains stored within the `df` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febafdf5-fa30-436a-9da9-9b0c9e2d294e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a429a17-6764-4225-9954-c26c0da4ab50",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769c547-80db-4947-ba0f-47189acd3c30",
   "metadata": {},
   "source": [
    "Now that we have an idea of what our dataset contains, and are sure that there are no missing values, we generally want to split it into three different subsets:\n",
    "* _Metadata_: Elements which are useful to data management, but are probably not informative in any way (i.e. patient IDs, software versions, protocol name etc.)\n",
    "* _Features_: Elements we expect to have available and intend to use when we want to make predictions using the final model (i.e. blood pressure, gender, genotype etc.)\n",
    "* _Target(s)_: The element(s) we want the machine learning model to predict for us, using the features designated prior (i.e. disease severity, surgical complication risk, blood sugar etc.)\n",
    "\n",
    "Use the dataframe view we have above and decide which rows/columns fit into each of these categories, and then group them accordingly. Once you have those elements, you can use `loc` or `iloc` to separate those elements into their respective groups, saving them as unique variables. If you don't feel comfortable with Python's querying format yet, lists (created using `[]`) can be used for this.\n",
    "\n",
    "For example, the command `df_meta = df.loc[:, ['feature 1', 'feature 2']]` will create a new subset dataframe named `df_meta` which contains all rows (`:`) and only columns `feature 1` and `feature 2` (`['feature 1', 'feature 2']`). For the sake of consistency with future commands, name these subsets `df_meta`, `df_feature`, and `df_target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286555f4-72e6-4add-8f3f-95b5419e69d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cdbbaf5-2dda-44e7-bf1b-d462340e5ce1",
   "metadata": {},
   "source": [
    "Finally, depending how the features are structured, we may want to make some final changes on the _features_ of the dataset to prepare it for use in our machine learning tool:\n",
    "\n",
    "* If the data distributions are different between features, you may want to normalize their values to all be roughly the same range and distribution (certain models rely on this to function properly). This can be done with `sklearn.preprocessing.normalize`, which will automatically scale each feature to within a -1 to +1 range. Its documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html)\n",
    "* If you suspect certain features may not be useful, and want to drop them, feature elimination may also be warranted. This can be done using `sklearn.feature_selection.VarianceThreshold`, `sklearn.feature_selection.SelectKBest`, or `sklearn.feature_selection.SelectPercentile`, depending on your preference and needs. Documentation for each are available [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html), [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), and [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html), respectively.\n",
    "* Finally, if you just have a ton of features and wish to reduce their number (either to reduce the change of overfitting, or because you suspect some features may be redundant), feature transformation can be used. While others exist, we recommend `sklearn.decomposition.PCA` (documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) or `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` (documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)) for unsupervised and supervised dimensionality reduction, respectively.\n",
    "\n",
    "If you want to employ any of these techniques, do so in the following code block. Just make sure that you are only applying them to your features (not metadata or target) and that the results remain saved to `df_feature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e18dca-c368-4426-ab95-33de059b3073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.092483</td>\n",
       "      <td>0.086268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.074034</td>\n",
       "      <td>0.080676</td>\n",
       "      <td>0.074766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.084398</td>\n",
       "      <td>0.079322</td>\n",
       "      <td>0.090515</td>\n",
       "      <td>0.080517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.087254</td>\n",
       "      <td>0.112160</td>\n",
       "      <td>0.120775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.079322</td>\n",
       "      <td>0.088547</td>\n",
       "      <td>0.086268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.089898</td>\n",
       "      <td>0.027548</td>\n",
       "      <td>0.017254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.088549</td>\n",
       "      <td>0.071390</td>\n",
       "      <td>0.104289</td>\n",
       "      <td>0.109273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.070563</td>\n",
       "      <td>0.100474</td>\n",
       "      <td>0.037387</td>\n",
       "      <td>0.023005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.100474</td>\n",
       "      <td>0.033451</td>\n",
       "      <td>0.017254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.081631</td>\n",
       "      <td>0.079322</td>\n",
       "      <td>0.082644</td>\n",
       "      <td>0.086268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0             0.092700          0.081966           0.092483          0.086268\n",
       "1             0.078864          0.074034           0.080676          0.074766\n",
       "2             0.084398          0.079322           0.090515          0.080517\n",
       "3             0.092700          0.087254           0.112160          0.120775\n",
       "4             0.074713          0.079322           0.088547          0.086268\n",
       "..                 ...               ...                ...               ...\n",
       "145           0.063645          0.089898           0.027548          0.017254\n",
       "146           0.088549          0.071390           0.104289          0.109273\n",
       "147           0.070563          0.100474           0.037387          0.023005\n",
       "148           0.078864          0.100474           0.033451          0.017254\n",
       "149           0.081631          0.079322           0.082644          0.086268\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63419c17-9ddc-4852-a615-6bd2b22b2eb8",
   "metadata": {},
   "source": [
    "Finally, we need to make sure the target is ready for analysis. This is much simpler; if its in a text-based form, we need to convert it to a numerical form instead. This can be done using `sklearn.preprocessing.LabelEncoder`, which will simply apply a single number to each unique value in the column. Its documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "\n",
    "If your data needs this done, do so in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a873701-9b0e-4e8d-bf1e-76d84bffe591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      2\n",
       "4      1\n",
       "      ..\n",
       "145    0\n",
       "146    2\n",
       "147    0\n",
       "148    0\n",
       "149    1\n",
       "Name: species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebbf0d90-1d0d-402d-849d-df39a702b1cd",
   "metadata": {},
   "source": [
    "## Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6cf05-0362-4ddf-bfdf-02a5e4c9cbe2",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3708e1-121b-4df3-a591-596ddbf7b483",
   "metadata": {},
   "source": [
    "Now that our dataset is ready, let's actually start building a model that makes use of it. Before we do so, it is common practice to split our data into two subsets:\n",
    "\n",
    "* _Training_: The data we will train the model on\n",
    "* _Testing_: The data the model will be evaluated using\n",
    "\n",
    "This is done to reduce the likelihood that the model will simply \"memorize\" the data it was trained on, making it appear to be extremely effective when in reality it performs poorly on new data. Thankfully this is very simple to do with SciKit-Learn by using the `sklearn.model_selection.train_test_split` function; by default it will provide a training dataset which contains 75% of your provided data, and a testing dataset with the remaining 25%. The documentation for its use can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Apply this to your features and data in the block below. For the variable names, append `_train` to the end of the training set data and `_test` for the testing set data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3295be6b-979c-4d17-b08b-1d721d146e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab1fc7ca-ced9-46e3-bb92-7d59d1afcb42",
   "metadata": {},
   "source": [
    "Now that we have our training set, we can look into which model to use. The biggest distinction here is whether your target is categorical or continuous. Categorical data is any data which can only exist as one of several distinct values (i.e. species, diabetes type, eye color etc.). In contrast, continuous metrics instead can exist as any value within a range (i.e. height, blood sugar, disease severity etc.). \n",
    "\n",
    "Note that these can occasionally be ambiguous and/or both simmultaneously; for example, one could measure eye color using hue, which would make it continuous rather than categorical. Likewise, continuous values can be \"binned\" to turn them into categorical values, should it better fit the use case needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ebe8-91e3-479d-9e9f-ae36ddbc9eb4",
   "metadata": {},
   "source": [
    "### Continuous Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b511fe2-b899-4fd9-aea5-35e75bf6bb73",
   "metadata": {},
   "source": [
    "Common models for working with continous targets are as follows:\n",
    "* `sklearn.linear_model.LinearRegression`: Provides a simple linear regression based prediction of your target, based on relations between the target and each of the features you provided. Very easy to interpret and extremely efficient to run, though its simplicity can lead to it missing trends other more advanced models would catch. Its full documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* `sklearn.svm.SVR`: A continuous regression implementation of the Support Vector Machine algorithm. Reasonably efficient and much more resistant to outliers than a simple linear regression, but its default `rbf` kernel makes it prone to overfitting if you are not careful. Other kernels can be used via the `kernel` argument to mitigate this somewhat, but this only helps so much. The full documentation for this tool can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "* `sklearn.ensemble.RandomForestRegressor`: A continuous regression implementation of the Random Forest method. As an ensemble machine learning system, this is incredibly resistant to overfitting, but generally less efficient and more difficult to interpret then the prior two. Its documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956d59a-4deb-449a-870d-6cdd54500767",
   "metadata": {},
   "source": [
    "### Categorical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854929bf-bb88-42a3-8575-662a18252df4",
   "metadata": {},
   "source": [
    "Common models for working with categorical targets are as follows:\n",
    "* `sklearn.linear_model.LogisticRegression`: The categorical equivalent to `LinearRegression` above, with the same benefits and drawbacks. Very easy to quickly train and test with, but extremely simplistic. Documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* `sklearn.svm.SVC`: A categorical-targetting implementation of the Support Vector Machine algorithm. Much like `SVR` prior, is very effective when outliers are a problem, but can overfit if not checked with its default kernel. Its documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "* `sklearn.ensemble.RandomForestClassifier`: A Random Forest built to predict categorial targets. Being an ensemble method like `RandomForestRegressor` prior, it shares the same benefits and drawbacks; resistant to overfitting, but less efficient and somewhat difficult to interpret. The documentation for this tool can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c748b-67e7-4caa-b877-96870e2b0503",
   "metadata": {},
   "source": [
    "Once you have selected a model from the list prior, we can fit it to our training data. Do so in the code block below, using the training datasets we created prior and saving the resulting model to the `model` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208ccf38-79f1-4350-965c-b5253b95e92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a04f445-cbef-4116-a425-01f169d95b33",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We have a working model! Now all thats left is to see how it performs. Depending on whether your target is continuous or categorical, this can be done in a number of ways, many of which you may want to perform together to get an accurate understanding of the model's performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720159d5-7136-4186-9600-b22f910a52d9",
   "metadata": {},
   "source": [
    "### Continuous Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243f801-84aa-4f3f-9dd2-43bb880d82c5",
   "metadata": {},
   "source": [
    "* `sklearn.metrics.mean_squared_error`: A good general use evaluation which calculates the error of the model as the sum of squared error (measured as difference between the true value, and the value predicted by the model). Penalizes large errors much more heavilly than small errors, making it very useful when these extreme mistakes would be more hazardous to the model's use than small mistakes. A value of 0 is a perfect model, with greater values represetning worse model performance. Its documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). \n",
    "* `sklearn.metrics.mean_absolute_error`: Another general use evaluation, though less common than the prior, which evaluates error as the absolute deviation between the true and model predicted values. This results in extreme error being penalized less severely than the prior method, making it better for evaluating models that need to treat slight errors as serious. A value of 0 is a perfect score, with increasing values representing worsening model performance. Its documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)\n",
    "* `sklearn.metrics.r2_score`: Useful in nearly all circumstances, calculates the _coefficient of determination_ (or $r^2$) of the model's predictions. This is similar to mean-squared error prior, but accounts for the precision of the model as well. Unlike the prior two, higher values are better, with a score of 1.0 being a perfect score and a score of 0.0 representing a 'naive' model (which does not utilize the features at all; if your model scores this or worse, it is not a very good model). Documentation for its use is provided [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2621ad-f03e-4979-9db8-728f7868b0af",
   "metadata": {},
   "source": [
    "### Categorical Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881aca64-b330-4157-a83a-c671eb731299",
   "metadata": {},
   "source": [
    "* `sklearn.metrics.accuracy_score`: Arguably the simplest measure of model performance, this simply reports how often the model predicts a sample's class correctly. Naturally this means a score of 1.0 is perfect (100% accuracy) and 0 is awful (0% accuracy). Its simplicity means it does not account for the circumstances of the data or model's use, however. Its documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "* `sklearn.metrics.balanced_accuracy_score`: Calculates accuracy, similar to the prior method, but accounts for target classes having different prevalance within the dataset. This makes its report more \"fair\", as classes which are rarer in the dataset will have a much more substantial effect on the final score if they are predicted incorrectly more often than classes which are more common. You can view the documentation for its use [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)\n",
    "* `sklearn.metrics.roc_auc_score`: Very common in medical applications, the ROC AUC evaluates how readilly the model can distinguish between true positives from false positives. This results in true and false negatives being ignored, making it useful for situations where false negatives are likely to be caught or accounted for in other ways. For data with more than two classes, this is calculated once per class, and reported back to you as a list. Documentation for its use can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "* `sklearn.metrics.recall_score`: Calculates the recall of the model, which is a measure of how reliably the model can identify true positives in the dataset. This results in the score ignoring true negatives and false positives. For data with more than two classes, this is calculated once per class, and reported back to you as a list. Its use is documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* `sklearn.metrics.precision_score`: Calculates the precision of the model, which is a measure of how reliable the model avoids misclassifying positive samples as negative. This results in false positives and true negatives being ignored. For data with more than two classes, this is calculated once per class, and reported back to you as a list. The documentation for its use is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "* `sklearn.metrics.classification_report`: Provides a generalized report of most of the metrics provided prior for convenience sake. By default will report Very useful if you just want a generalized overview of a model's performance, without needing to compare multiple models automatically within the code. Its documentation is provided [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9f9ff-2195-43c1-958f-8efc25e2d4b7",
   "metadata": {},
   "source": [
    "Place the code for the evaluation(s) you wish to perform in the code block below. If you think the model performed sufficiently, congrats! If not, you can use this to inform how you want to change the prior code blocks to account for the issues instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7612ec97-09f7-4da7-b60d-19c299b135eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.87      1.00      0.93        13\n",
      "           2       1.00      0.86      0.92        14\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.96      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDPR_Course",
   "language": "python",
   "name": "mdpr_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
